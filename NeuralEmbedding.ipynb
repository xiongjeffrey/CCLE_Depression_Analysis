{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLbQfF9eMzbX6Rr3RN07VB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiongjeffrey/CCLE_Depression_Analysis/blob/main/NeuralEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference paper: https://github.com/geomstats/challenge-iclr-2021/blob/main/gcorso__Neural-Sequence-Distance-Embeddings/Neural_SEED.ipynb"
      ],
      "metadata": {
        "id": "Bx0oQu5FAfAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzgZrRomfP86"
      },
      "outputs": [],
      "source": [
        "# INITIAL SET UP: ONLY NEED TO RUN ONCE PER RUNTIME\n",
        "\n",
        "!pip3 install geomstats\n",
        "!apt install clustalw\n",
        "!pip install biopython\n",
        "!pip install python-Levenshtein\n",
        "!pip install Cython\n",
        "!pip install networkx\n",
        "!pip install tqdm\n",
        "!pip install gdown\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/gcorso/neural_seed.git\n",
        "import os\n",
        "os.chdir(\"neural_seed\")\n",
        "!cd hierarchical_clustering/relaxed/mst; python setup.py build_ext --inplace; cd ../unionfind; python setup.py build_ext --inplace; cd ..; cd ..; cd ..;\n",
        "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'\n",
        "\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rVxNYspFtXD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIAL SET UP CONTINUED: ONLY NEED ONCE PER RUNTIME\n",
        "import torch\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "from edit_distance.train import load_edit_distance_dataset\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from util.ml_and_math.loss_functions import AverageMeter"
      ],
      "metadata": {
        "id": "qCJPu83QALTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from geomstats.geometry.poincare_ball import PoincareBall"
      ],
      "metadata": {
        "id": "366QYMdZtvIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT DATASET HERE\n",
        "# commented-out portion is reference from neuralseed\n",
        "\n",
        "!wget https://github.com/bhattlab/SupplementaryInformation/raw/master/SmORFinder/datasets.tar.gz\n",
        "!tar -zxvf datasets.tar.gz -C '/content/'\n",
        "\n",
        "# !gdown --id 1yZTOYrnYdW9qRrwHSO5eRc8rYIPEVtY2 # for edit distance approximation\n",
        "# !gdown --id 1hQSHR-oeuS9bDVE6ABHS0SoI4xk3zPnB # for closest string retrieval\n",
        "# !gdown --id 1ukvUI6gUTbcBZEzTVDpskrX8e6EHqVQg # for hierarchical clustering"
      ],
      "metadata": {
        "id": "T9IZ857bATN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing\n",
        "df = pd.read_csv(\"/content/datasets/dataset_FINAL.tsv\", sep='\\t')\n",
        "df.head()\n",
        "\n",
        "# save subset of dataset\n",
        "\n",
        "df_subset=df.sample(10000, random_state=42)\n",
        "df_subset.to_csv('smorf_subset.csv')\n",
        "\n",
        "# get only protein sequence from training subset\n",
        "train=df_subset[df_subset.set=='train']\n",
        "x=train.smorf.tolist()\n",
        "y=train.y=='positive'\n",
        "\n",
        "# encode the protein sequence as a one-hot\n",
        "def onehote(sequence):\n",
        "  seq_array = np.array(list(sequence))\n",
        "\n",
        "  #integer encode the sequence\n",
        "  label_encoder = LabelEncoder()\n",
        "  integer_encoded_seq = label_encoder.fit_transform(seq_array) \n",
        "\n",
        "  #one hot the sequence\n",
        "  onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "  #reshape because that's what OneHotEncoder likes\n",
        "  integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
        "  onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
        "  return onehot_encoded_seq"
      ],
      "metadata": {
        "id": "4v3o3jMDU47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshteinDist(seq1, seq2): #!TODO: figure out how to implement this properly taking in numerics\n",
        "  return calculate_levenshtein_distance(str_1, str_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKNRn2OGqXRG",
        "outputId": "a15cdb35-e201-4a55-bbc7-58dcac7b3f19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generation\n",
        "class DistGenerator():\n",
        "\n",
        "  def __init__(self, seqA, seqB, func, batchSize = 32):\n",
        "    self.seqA = seqA\n",
        "    self.seqB = seqB\n",
        "    self.batchSize = batchSize\n",
        "    self.dist = func\n",
        "\n",
        "  def __data_generation(self, seqA, seqB):\n",
        "      '''\n",
        "      Generate data containing batch_size samples.\n",
        "          \n",
        "      Inputs: two naked sequences (numpy arrays) + parameter for batch size (default 32)\n",
        "              \n",
        "      Returns: tuple of aBatches, bBatches, batchDist\n",
        "          batchSeqs : tuple of size 2, i.e. (aBatches, bBatches), containing sequences contained in each batch\n",
        "            each batch sequence will contain an array of numpy arrays containing each sequence\n",
        "          batchDist: dist between the pairs of samples in the corresponding entries of X0 and X1, array of length batch_size\n",
        "      '''\n",
        "      # Initialization\n",
        "      aBatches = []\n",
        "      bBatches = []\n",
        "\n",
        "      batchDist = [0]*self.batchSize\n",
        "\n",
        "      # Generate data\n",
        "      # if there's a remainder at the end, it will go in one slightly longer batch\n",
        "      for i in range(len(seqA)/self.batchSize):\n",
        "\n",
        "          # Store sample pairings\n",
        "          aBatches.append(seqA[i*self.batchSize[i][0]:])\n",
        "          bBatches.append(seqB[i*self.batchSize[i][1]:])\n",
        "\n",
        "          # Store distance between the pair\n",
        "          batchDist[i] = self.dist(aBatches[-1], bBatches[-1])\n",
        "\n",
        "      return (aBatches, bBatches, batchDist)"
      ],
      "metadata": {
        "id": "n8ChX9_jxt9l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence transformation\n",
        "def seqTransform(seqA, seqB): # takes in onehote\n",
        "      sequence = np.zeros((max(len(seqA), len(seqB)), 4))\n",
        "        # WLOG, seqA is longer      \n",
        "      if len(seqA) < len(seqB):\n",
        "        temp = seqA\n",
        "        seqA = seqB\n",
        "        seqB = temp\n",
        "      \n",
        "      firstLimit = len(seqB)\n",
        "      secondLimit = len(seqA)\n",
        "\n",
        "      i = 0\n",
        "\n",
        "      for i in range(firstLimit):\n",
        "        sequence[i] = np.subtract(seqA[i], seqB[i])\n",
        "      for i in range(secondLimit):\n",
        "        sequence[i] = seqA[i]\n",
        "\n",
        "      # defining basic transformation\n",
        "      # sequence has dimensions n * 4\n",
        "      seqTensor = sequence.astype(np.float32)\n",
        "      seqTensor = torch.tensor(sequence, dtype=torch.float32)\n",
        "      return seqTensor"
      ],
      "metadata": {
        "id": "fo3EeJWLafBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LevenshteinNN(nn.Module): # Linear layers so far\n",
        "\n",
        "    def __init__(self, sequence): \n",
        "      super(ConvNet, self).__init__()\n",
        "      self.sequence = sequence\n",
        "      \n",
        "      self.base = nn.Sequential( \n",
        "          \n",
        "        #!TODO: think more deeply about architecture!\n",
        "        # can refer to smORF paper for ideas: https://www.sciencedirect.com/science/article/pii/S1931312820306193\n",
        "        self.conv1 = nn.Conv1d(len(sequence), 35, 3, stride=2) # in_channels, out_channels, kernel_size, stride=1\n",
        "        self.drop1 = nn.Dropout(p = 0.1)\n",
        "        self.pool = nn.MaxPool1d(3, 2)\n",
        "        self.conv2 = nn.Conv1d(35, 10, 5)\n",
        "        self.drop2 = nn.Dropout(p = 0.1)\n",
        "        self.fc1 = nn.Linear(10 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "      )\n",
        "\n",
        "    def forward(self, features):\n",
        "      # x = self.layer1(features.float())\n",
        "      # x = self.layer2(x)\n",
        "      # x = self.layer3(x)\n",
        "        \n",
        "      x = self.base(features.float())\n",
        "\n",
        "      return x # returns 32 * 1 size matrix"
      ],
      "metadata": {
        "id": "lR4cuzDlF1nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testHotA = onehote(x[1])\n",
        "testHotB = onehote(x[2])\n",
        "testSeq = seqTransform(testHotA, testHotB)\n",
        "\n",
        "features = torch.from_numpy(testHotB.transpose())\n",
        "\n",
        "features = features.to(\"cuda\")\n",
        "test = LevenshteinNN(testSeq)\n",
        "test.to(\"cuda\")\n",
        "test(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uVI5pBWMGFA",
        "outputId": "4d7b5072-b9b3-49ab-ddf9-d0c5d4b4aa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3459],\n",
              "        [-0.3326],\n",
              "        [-0.3926],\n",
              "        [-0.3695]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperbolicLoss(x, y): # finds the hyperbolic distance between two vectors of the same length\n",
        "  #!TODO: implement distance via this: https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model#Distance\n",
        "  sum = 1\n",
        "  for i in range(len(x)):\n",
        "    sum += ((y[i] - x[i])**2)/(2*x[-1]*y[-1])\n",
        "  return torch.arccosh(sum)"
      ],
      "metadata": {
        "id": "OdwMHy88eJKN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEncoder(nn.Module):\n",
        "    \"\"\"  Linear model which simply flattens the sequence and applies a linear transformation. \"\"\"\n",
        "\n",
        "    def __init__(self, len_sequence, embedding_size, alphabet_size=4):\n",
        "        super(LinearEncoder, self).__init__()\n",
        "        self.encoder = nn.Linear(in_features=alphabet_size * len_sequence, \n",
        "                                 out_features=embedding_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten sequence and apply layer\n",
        "        B = sequence.shape[0]\n",
        "        sequence = sequence.reshape(B, -1)\n",
        "        emb = self.encoder(sequence)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class PairEmbeddingDistance(nn.Module):\n",
        "    \"\"\" Wrapper model for a general encoder, computes pairwise distances and applies projections \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model, embedding_size, scaling=False):\n",
        "        super(PairEmbeddingDistance, self).__init__()\n",
        "        self.hyperbolic_metric = PoincareBall(embedding_size).metric.dist\n",
        "        self.embedding_model = embedding_model\n",
        "        self.radius = nn.Parameter(torch.Tensor([1e-2]), requires_grad=True)\n",
        "        self.scaling = nn.Parameter(torch.Tensor([1.]), requires_grad=True)\n",
        "\n",
        "    def normalize_embeddings(self, embeddings):\n",
        "        \"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"\n",
        "        min_scale = 1e-7\n",
        "        max_scale = 1 - 1e-3\n",
        "        return F.normalize(embeddings, p=2, dim=1) * self.radius.clamp_min(min_scale).clamp_max(max_scale)\n",
        "\n",
        "    def encode(self, sequence):\n",
        "        \"\"\" Use embedding model and normalization to encode some sequences. \"\"\"\n",
        "        enc_sequence = self.embedding_model(sequence)\n",
        "        enc_sequence = self.normalize_embeddings(enc_sequence)\n",
        "        return enc_sequence\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten couples\n",
        "        (B, _, N, _) = sequence.shape\n",
        "        sequence = sequence.reshape(2 * B, N, -1)\n",
        "\n",
        "        # encode sequences\n",
        "        enc_sequence = self.encode(sequence)\n",
        "\n",
        "        # compute distances\n",
        "        enc_sequence = enc_sequence.reshape(B, 2, -1)\n",
        "        distance = self.hyperbolic_metric(enc_sequence[:, 0], enc_sequence[:, 1])\n",
        "        distance = distance * self.scaling\n",
        "\n",
        "        return distance"
      ],
      "metadata": {
        "id": "XFd6z6CKAino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, loss, device):\n",
        "  # model is the particular model we are training on (e.g. Levenshtein)\n",
        "  # data is the set of data that we're working on (e.g. reads) AND the target (e.g. real Levenshtein distance)\n",
        "  # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  # loss is chosen loss function\n",
        "  # device is usually cuda\n",
        "\n",
        "\n",
        "  avg_loss = AverageMeter()\n",
        "  model.train()\n",
        "\n",
        "  for sequences, target in data:\n",
        "    # move to device\n",
        "    sequences, target = sequences.to(device), target.to(device)\n",
        "\n",
        "    #forward\n",
        "    optimizer.zero_grad()\n",
        "    output = model(sequences)\n",
        "\n",
        "    # loss and backpropagation\n",
        "    loss_train = loss(output, target)\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # keep track of average loss\n",
        "  avg_loss.update(loss_train.data.item(), sequences.shape[0])\n",
        "\n",
        "def test(model, data, loss, device):\n",
        "  avg_loss = AverageMeter()\n",
        "  model.eval()\n",
        "\n",
        "  for sequences, target in data:\n",
        "    # move examples to right device\n",
        "    sequences, target = sequences.to(device), target.to(device)\n",
        "\n",
        "    # forward propagation and loss computation\n",
        "    output = model(sequences)\n",
        "    loss_val = loss(output, target).data.item()\n",
        "    avg_loss.update(loss_val, sequences.shape[0])\n",
        "\n",
        "  return avg_loss.avg\n"
      ],
      "metadata": {
        "id": "_1ikbHNilt0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification of results: via https://github.com/python-engineer/pytorchTutorial/blob/master/14_cnn.py\n",
        "with torch.no_grad():\n",
        "\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    trialNum = 0 # can be set to desired number of verification trials per item\n",
        "\n",
        "    n_class_correct = [0 for i in range(trialNum)]\n",
        "    n_class_samples = [0 for i in range(trialNum)]\n",
        "\n",
        "    for seqs, targets in loader:\n",
        "\n",
        "        seqs = seqs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = LevenshteinNN(seqs)\n",
        "\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += targets.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            target = targets[i]\n",
        "            pred = predicted[i]\n",
        "            if (target == pred):\n",
        "                n_class_correct[target] += 1\n",
        "            n_class_samples[target] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "id": "DjYd2zBSBi9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Driver Code\n",
        "#!TODO: clean up this mess :(\n",
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(2021)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(2021)\n",
        "\n",
        "# load data\n",
        "datasets = onehote(x[1])\n",
        "# datasets = load_edit_distance_dataset('./edit_qiita_large.pkl')\n",
        "loaders = get_dataloaders(datasets, batch_size=128, workers=1)\n",
        "\n",
        "# model, optimizer and loss\n",
        "encoder = LinearEncoder(152, EMBEDDING_SIZE)\n",
        "model = PairEmbeddingDistance(embedding_model=encoder, embedding_size=EMBEDDING_SIZE)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss = hyperbolicLoss()\n",
        "\n",
        "# training\n",
        "for epoch in range(0, 21):\n",
        "    t = time.time()\n",
        "    loss_train = train(model, loaders['train'], optimizer, loss, device)\n",
        "    loss_val = test(model, loaders['val'], loss, device)\n",
        "\n",
        "    # print progress\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {:02d}'.format(epoch),\n",
        "              'loss_train: {:.6f}'.format(loss_train),\n",
        "              'loss_val: {:.6f}'.format(loss_val),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "      \n",
        "# testing\n",
        "for dset in loaders.keys():\n",
        "    avg_loss = test(model, loaders[dset], loss, device)\n",
        "    print('Final results {}: loss = {:.6f}'.format(dset, avg_loss))"
      ],
      "metadata": {
        "id": "aVeTJJDhAxgT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "7c5e787c-39e7-40a4-ef4f-b41b3154bbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e82009575727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# datasets = load_edit_distance_dataset('./edit_qiita_large.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# model, optimizer and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/neural_seed/util/data_handling/data_loader.py\u001b[0m in \u001b[0;36mget_dataloaders\u001b[0;34m(datasets, batch_size, workers)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         loaders[key] = torch.utils.data.DataLoader(datasets[key], batch_size=batch_size,\n\u001b[1;32m     16\u001b[0m                                                    shuffle=True, num_workers=workers)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hierarchical_clustering.unsupervised.unsupervised import hierarchical_clustering_testing\n",
        "\n",
        "hierarchical_clustering_testing(encoder_model=model, data_path='./hc_qiita_large_extr.pkl',\n",
        "                                batch_size=128, device=device, distance='hyperbolic')"
      ],
      "metadata": {
        "id": "EXOw4DvmA6z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiple_alignment.guide_tree.guide_tree import approximate_guide_trees\n",
        "\n",
        "# performs neighbour joining algorithm on the estimate of the pairwise distance matrix\n",
        "approximate_guide_trees(encoder_model=model, dataset=datasets['test'],\n",
        "                        batch_size=128, device=device, distance='hyperbolic')\n",
        "\n",
        "# Command line clustalw using the tree generated with the previous command. \n",
        "# The substitution matrix and gap penalties are set to simulate the classical edit distance used to train the model \n",
        "!clustalw -infile=\"sequences.fasta\" -dnamatrix=multiple_alignment/guide_tree/matrix.txt -transweight=0 -type='DNA' -gapopen=1 -gapext=1 -gapdist=10000 -usetr"
      ],
      "metadata": {
        "id": "wGztyOt5A9Dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}